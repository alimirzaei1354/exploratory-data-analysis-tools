---
title: "Exploratory Data Analysis"
description: |
  A Demonstration of Tools
author:
  - name: Michael Clark
    url: https://m-clark.github.io
date: '`r Sys.Date()`'
# preview: 
output:
  distill::distill_article:
    self_contained: false
    toc: true
    css: distill.css
bibliography: cites.bib
tags: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  comment = NA,
  R.options = list(width = 120),
  cache.rebuild = FALSE,
  cache = FALSE,
  fig.align = 'center',
  fig.asp = .7,
  dev = 'svglite',
  dev.args = list(bg = 'transparent')
)

library(tidyverse)
library(kableExtra)
library(scico)
library(noiris)

kable_df <- function(..., digits=3) {
  kable(..., digits=digits) %>% 
    kable_styling(full_width = F)
}

rnd = tidyext::rnd
```

## Introduction

In R there are many tools available for exploring data.  However, in consulting I still see a lot of people using base R's <span class="func" style = "">table</span> and <span class="func" style = "">summary</span> functions, followed by a lot of work to get the result in a more presentable format.  My own frustrations led to me creating a mini package ([tidyext](https://m-clark.github.io/tidyext/)) for personal use in this area.  While that suits me fine, there are tools that can go much further with little effort. Recently, Staniak & Biecek @staniak2019landscape wrote an article in the R Journal exploring several of such packages, so I thought I'd try them out for myself, and take others along with me for that ride.

## Packages

```{r load-data}
pkg_downloads = read_rds('data/cran_downloads_pkgs.rds')
monthly_stats = read_rds('data/monthly_stats.rds')
```

I've updated the article's table 1, basically as it would have a year's worth of downloads.

```{r table-1}
pkg_downloads %>% 
  group_by(package) %>% 
  summarise(
    downloads = sum(count),
    debut = min(date)
    ) %>% 
  arrange(desc(downloads)) %>% 
  kable_df()
```

A more informative assessment of usage would be in average monthly downloads.

```{r avg-monthly-downloads}
monthly_stats %>% 
  distinct(package, average_monthly_downloads) %>% 
  arrange(desc(average_monthly_downloads)) %>% 
  kable_df()
```

Here is a visualization of their growth over time.


```{r cran-packs, echo=FALSE}
monthly_stats %>%
  ggplot(aes(x = year_mo, y = monthly_downloads, color = package)) +
  geom_hline(aes(yintercept = 5000), color = 'gray92') +
  ggbump::geom_bump(size = .5, alpha = .5, show.legend = F) +
  # ggforce::geom_bspline0(size = .5) +
  ggrepel::geom_text_repel(
    aes(label = package),
    size = 2,
    show.legend = F,
    data = . %>%
      group_by(package) %>%
      filter(year == last(year), month  == last(month))
  ) +
  scico::scale_color_scico_d() +
  guides(x = guide_axis(n.dodge = 2)) +
  scale_x_discrete(breaks = paste0(2016:2020, c('-Jan', '-Jun'))) +
  scale_y_continuous(breaks = c(1000, 2500, 5000, 10000, 15000)) +
  labs(x = '', y = '', subtitle = "CRAN monthly downloads") +
  visibly::theme_clean()
```



### Selected packages

I'll outline my reasons for selecting some packages to explore and not others. These reasons are somewhat, but not necessarily, arbitrary, and may leave out some viable newer packages.  For the data scenario, I am assuming messy data of the sort that might have hundreds of columns of mixed data types, potentially with lots of missingness, attributes that are only applicable to subsets of the data (e.g. branching logic in surveys), etc.[^nottoomessy]

Here is my criteria for selection:


- Relatively more widely used
- I've used it before and want to revisit it
- I've heard of it before and want to try it
- I'm familiar with the package author's work
- Appears to be in active development (especially on GitHub) according to modern programming standards
- Good documentation

Things I'm not as concerned about:

- Anything that requires actual analysis.  I have no interest in bivariate statistical tests, PCA, imputation.  It likely isn't appropriate, and I could do better via other means.
- Visuals, though fundamental for data exploration, can usually be done better with even a modicum of effort, so this wouldn't be a big factor for me, and some common ones are poor for communication (e.g. histograms). However, if it saves me some time for initial presentations or something like that, all the better.


In the end, I would like a package that is well will make common tasks easier for me and potentially save me time in creating reports/presentation.


Staniak & Biecek note two general phases of data exploration, each with specific tasks, based on the CRISP-DM standard @wirth2000crisp [^cripdm].

- Understanding
    - Description
    - Validity
    - Exploration

- Preparation
    - Cleaning
    - Derived Attributes

In the first, I will focus on tools for understanding, particularly description and validity, as they refer to exploration tasks as visualization, which is a perk, but something I'm more inclined to do myself.

I'm definitely less interested in the preparation.  'Cleaning' in the article refers to mean/median imputation, something I've never bothered to do for reasons that have been noted in the statistical literature for a long time.  The other transformations are easy and should be explicitly documented.  Furthermore, in creating derived attributes, things like category merging and standardization depend on the subset of the data used, so it would be better to be more explicit than automatic.  Also, if things like an automated PCA is viable for your situation, it probably is simple data (i.e. all variables of the same type), in which case, most of these tools probably won't add much value to you.

So with that in mind here are the ones I will explore (in alphabetical order):

- arsenal
- DataExplorer
- dataMaid
- SmartEDA
- summarytools
- visdat

## The Data

I've chosen the heart_disease data available from the [UCI repository](https://archive.ics.uci.edu/ml/datasets/Heart+Disease). It contains a mixture of data types  It's been cleaned and I've additionally added some random missingness.

```{r prep-data}
hd = noiris::heart_disease
hd_mat = as.matrix(hd)
hd_mat[sample(prod(dim(hd)), .05*prod(dim(hd)))] = NA
hd = as_tibble(hd_mat) %>% 
  mutate_at(
    vars(
      age,
      resting_bp,
      cholesterol,
      resting_ecg,
      max_heartrate,
      old_peak,
      n_vessels,
      heart_disease
    ),
    as.numeric
  ) %>% 
  mutate(id = 1:n()) %>% 
  select(id, everything())

hd %>% 
  DT::datatable(options = list(dom='tp', scrollX=T), rownames = F)
```


```{r set_echo_true}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Description

For data description
dimensions variables number
variables variable type
meta-data size in RAM

### Preliminary

To give a sense of what my preference is, consider my own functions. The following actually calls a separate numerical summary function as well as a categorical variable function, and both return 'tidy' data frames that can immediately be used for presentation (e.g. kableExtra) and visualization, and drilling down to only selections of the output.

```{r tidyext, echo=TRUE}
library(tidyext)

describe_all(hd, extra = T)
```

I also have options, such as the following.

```{r tidyext-options}
hd %>% 
  select(age, sex, heart_disease) %>% 
  describe_all(
  digits = 1,
  include_NAcat = TRUE,
  max_levels = 3,
  include_numeric = TRUE,
  sort_by_freq = TRUE,
  extra = TRUE
)
```

For grouped output, I can use the underlying functions.  Here we look at summaries for numerical variables.

```{r numby}
hd %>% 
  num_by(
    main_var  = vars(age, cholesterol),
    group_var = sex,
    extra = TRUE
  )
```

The categorical variable functionality is very similar.

```{r catby}
hd %>% 
  cat_by(
    main_var  = chest_pain_type,
    group_var = sex,
    perc_by_group = TRUE
  )
```

As these are tidy tibbles, they are essentially ready for presentation.

```{r demokableExtra}
describe_all_num(hd) %>% 
  kableExtra::kable()
```

These serve most of my needs for initial peeking at the data.

### arsenal

We begin alphabetically with the arsenal package.  Here we use table by for a generic summary as well as grouped summary. The result is markdown, so for presentation one must use `results='asis'`

```{r arsenal-table-by, echo = T, results='asis'}
library(arsenal)
hd %>% 
  select(age, sex, heart_disease) %>% 
  tableby(~., data = .) %>% 
  summary()
# summary(arsenal::freqlist(hd))
```

```{r arsenal-cats, results='asis'}
with(hd, table(sex, chest_pain_type)) %>% 
  freqlist() %>% 
  summary()
```



#### Pros

The tableby summary is essentially a 'Table 1', which is an unfortunately named display of descriptive stats for a sample of a given study. My clients often want that, and I've actually used a package specifically geared towards that just to save me the headache (tableOne), so I definitely find that aspect useful.  However, Table 1's almost invariably have needless statistical output or analysis, and are not a model I would choose to go for.  As you can see, the layout is not going to be viable for more than a few variables, though it is common practice to present them that way in journal articles regardless of verbosity/legibility.

#### Issues

I'm not thrilled with markdown output, as I have little control over it, but it's fine.  Also, the default layout is not so succinct, and it appears it's trying to emulate functions like SAS's PROC FREQ, which is not a model for presentation in my opinion. One can use a function to write the output to html, but that's not something I want or need.


### DataExplorer

Now we move to DataExplorer.  Let's introduce the package with the introduce function.  I already like this, as it provides good info that extends what you'd get with str or glimpse.

```{r data-explorer-introduce}
library(DataExplorer)

introduce(hd)
```

It also can display this information visually in two different ways.  I like this, but can't say I'd ever have a reason to actually use it.

```{r data-explorer-plot_str}
plot_str(list(hd, gapminder_2019))
```

```{r data-explorer-plot_intro}
plot_intro(hd)
```

We can focus on the missingness.

```{r data-explorer-plot_missing}
plot_missing(hd)
```

It can also plot distributions, e.g. bar and histograms,


```{r data-explorer-plot_bar}
plot_bar(hd)
```

But this sort of thing only takes a couple lines of ggplot to do on your own, which you can then customize far more easily.

```{r data-explorer-plot_intro-ggplot}
hd %>% 
  select_if(is.character) %>% 
  pivot_longer(everything(), names_to = 'variable', values_to = 'value') %>% 
  drop_na() %>% 
  ggplot(aes(x = value)) +
  geom_bar() +
  coord_flip() +
  facet_wrap(~ variable, scales = 'free') 
```


Similarly there are QQ plots, scatterplots and more. I always try to get people to try correlation plots in lieu of large correlation matrices, and DataExplorer provides this.  The nice thing is that it will automatically create indicator variables for levels of categorical variables, but beyond that there are issues.   For one, it's diagonal is reversed from the typical presentation of correlation matrices. If you don't drop the missing, the plot isn't very useful, because it will include NA as a factor level, but to its credit DataExplorer has an option to only focus on continuous or discrete output. In addition, 'centered' is not the obvious choice for alignment of the x axis, so this would take additional work to make presentable.

```{r data-explorer-plot_corr}
plot_correlation(
  hd,
  ggtheme = theme_minimal(),
  cor_args = list("use" = "pairwise.complete.obs")
)
```

There are packages that do this specifically, such as heatmaply. I have my own function that does an internal factor analysis to sort the variables and produces an interactive result, so this aspect of DataExplorer doesn't appeal to me.  

```{r visibly-corrheat}
hd %>% 
  select_if(is.numeric) %>% 
  select(-id) %>% 
  cor(use = 'pair') %>% 
  visibly::corr_heat()
```

If the various options of output, or only some of them, appeals to you, they can all be nicely wrapped up in an automatic report. Various settings for each type of output can be set with an additional function (configure_report) or just passing a list of arguments, including which functions to use, ggplot2 theme, etc.  You can see the report [here]().

<!-- ADD LINK -->

```{r data-explorer-report, eval=F}
create_report(hd, output_dir = 'other_docs', output_file = 'data_explorer_report.html')
```



### SmartEDA
### summarytools
### visdat

## Data Validity

### dataMaid
### janitor


## Exercises




[^nottoomessy]: However, for our demonstration I'll be using something a little more wieldy.

 [^cripdm]: To be honest, I wasn't familiar with the CRoss Industry Standard Process for Data Mining until reading the article citing it. I don't get the impression any particular methodology is actually consciously thought about by the vast majority of practicing data scientists, but it's useuful in providing a framework for the content here.